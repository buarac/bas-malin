# F5.4 - Computer Vision Avanc√©e

**Score Priorit√© :** 45/100  
**Statut :** PRIORIT√â FAIBLE  
**Epic :** EPIC 5 - IA AVANC√âE & PR√âDICTIONS  
**Effort estim√© :** 25 jours  

## Description

Syst√®me de computer vision avanc√© pour reconnaissance automatique de maladies, identification des stades de croissance, suivi √©volution temporelle des cultures et g√©n√©ration de time-lapse avec analyse intelligente des transformations.

## User Stories

### US5.4.1 - Diagnostic Automatique Maladies
**En tant qu'** jardinier  
**Je veux** diagnostiquer automatiquement les maladies de mes plantes avec une photo  
**Afin de** d√©tecter pr√©cocement les probl√®mes et agir rapidement  

**Crit√®res d'acceptation :**
- Reconnaissance 20+ maladies communes (>80% pr√©cision)
- Diagnostic avec niveau de confiance et zones affect√©es
- Recommandations traitement sp√©cifiques √† la maladie
- D√©tection pr√©coce avant sympt√¥mes visibles humains
- Base de donn√©es sympt√¥mes √©volutive

### US5.4.2 - Suivi Croissance Automatique
**En tant qu'** utilisateur  
**Je veux** suivre automatiquement l'√©volution de mes cultures  
**Afin de** comprendre leur d√©veloppement et optimiser les soins  

**Crit√®res d'acceptation :**
- Reconnaissance stades croissance par esp√®ce
- Mesures automatiques (taille, volume, maturit√©)
- Comparaison avec standards de croissance
- Alertes croissance anormale ou retard
- Historique visuel avec m√©triques

### US5.4.3 - Time-lapse Intelligent
**En tant qu'** passionn√©  
**Je veux** cr√©er des time-lapse intelligents de mes cultures  
**Afin de** visualiser et partager l'√©volution de mon jardin  

**Crit√®res d'acceptation :**
- Alignement automatique des photos s√©quentielles
- G√©n√©ration time-lapse avec points d'int√©r√™t
- Analyse narrative de l'√©volution
- D√©tection √©v√©nements marquants (floraison, fructification)
- Export vid√©o haute qualit√© avec m√©tadonn√©es

## Architecture Technique

### Service Computer Vision

```typescript
// src/services/vision/computer-vision.service.ts
export interface PlantAnalysis {
  species?: {
    name: string;
    confidence: number;
    family: string;
    characteristics: string[];
  };
  health: {
    overall: 'excellent' | 'good' | 'fair' | 'poor' | 'critical';
    score: number; // 0-100
    issues: HealthIssue[];
  };
  growth: {
    stage: GrowthStage;
    progress: number; // % vers next stage
    metrics: GrowthMetrics;
  };
  recommendations: VisionRecommendation[];
}

export interface HealthIssue {
  type: 'disease' | 'pest' | 'deficiency' | 'environmental';
  name: string;
  severity: 'low' | 'medium' | 'high' | 'critical';
  confidence: number;
  affectedAreas: BoundingBox[];
  symptoms: string[];
  causes: string[];
  treatments: Treatment[];
}

export interface GrowthStage {
  name: string;
  description: string;
  typicalDuration: string;
  nextStage?: string;
  characteristics: string[];
}

export interface GrowthMetrics {
  estimatedHeight: number;
  leafCount?: number;
  flowerCount?: number;
  fruitCount?: number;
  biomassEstimate: number;
  maturityPercentage: number;
}

export class ComputerVisionService {
  private openaiClient: OpenAI;
  private plantNetClient: PlantNetAPI;
  private diseaseModels: Map<string, MLModel>;
  private growthModels: Map<string, MLModel>;

  constructor(
    private prisma: PrismaClient,
    private storageService: StorageService,
    private mlService: MLService
  ) {
    this.openaiClient = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
    this.plantNetClient = new PlantNetAPI();
    this.diseaseModels = new Map();
    this.growthModels = new Map();
    this.initializeModels();
  }

  async analyzeImage(
    imageBuffer: Buffer,
    metadata: ImageMetadata,
    analysisTypes: ('species' | 'health' | 'growth')[] = ['species', 'health', 'growth']
  ): Promise<PlantAnalysis> {
    // Upload et pr√©paration de l'image
    const imageUrl = await this.uploadAnalysisImage(imageBuffer, metadata);
    const processedImage = await this.preprocessImage(imageBuffer);

    const analysis: Partial<PlantAnalysis> = {};

    // Analyse d'esp√®ce si demand√©e
    if (analysisTypes.includes('species')) {
      analysis.species = await this.identifySpecies(processedImage, imageUrl);
    }

    // Analyse de sant√© si demand√©e
    if (analysisTypes.includes('health')) {
      analysis.health = await this.analyzeHealth(
        processedImage, 
        imageUrl, 
        analysis.species?.name
      );
    }

    // Analyse de croissance si demand√©e
    if (analysisTypes.includes('growth')) {
      analysis.growth = await this.analyzeGrowth(
        processedImage, 
        imageUrl, 
        analysis.species?.name
      );
    }

    // G√©n√©ration des recommandations globales
    analysis.recommendations = await this.generateRecommendations(
      analysis as PlantAnalysis,
      metadata
    );

    // Sauvegarde de l'analyse
    await this.saveAnalysis(imageUrl, analysis, metadata);

    return analysis as PlantAnalysis;
  }

  private async identifySpecies(
    processedImage: ProcessedImage,
    imageUrl: string
  ): Promise<PlantAnalysis['species']> {
    // Utilisation combin√©e de GPT-4 Vision et PlantNet
    const [visionResult, plantNetResult] = await Promise.all([
      this.identifyWithVision(imageUrl),
      this.identifyWithPlantNet(processedImage.buffer)
    ]);

    // Fusion des r√©sultats pour am√©liorer la pr√©cision
    return this.fusionSpeciesResults(visionResult, plantNetResult);
  }

  private async identifyWithVision(imageUrl: string): Promise<any> {
    const completion = await this.openaiClient.chat.completions.create({
      model: 'gpt-4-vision-preview',
      messages: [
        {
          role: 'user',
          content: [
            {
              type: 'text',
              text: `Identifie cette plante avec le maximum de pr√©cision. Fournis:
1. Nom scientifique et nom commun
2. Famille botanique
3. Caract√©ristiques distinctives visibles
4. Niveau de confiance de l'identification
5. Plante potag√®re ou ornementale?

Format ta r√©ponse en JSON avec les champs: scientificName, commonName, family, characteristics, confidence, type`
            },
            {
              type: 'image_url',
              image_url: { url: imageUrl }
            }
          ]
        }
      ],
      max_tokens: 500
    });

    return JSON.parse(completion.choices[0].message.content || '{}');
  }

  private async identifyWithPlantNet(imageBuffer: Buffer): Promise<any> {
    try {
      const result = await this.plantNetClient.identify({
        images: [imageBuffer],
        modifiers: ['crops', 'useful'],
        includeRelatedImages: false
      });

      return result.results[0]; // Meilleur match
    } catch (error) {
      console.warn('PlantNet identification failed:', error);
      return null;
    }
  }

  private async analyzeHealth(
    processedImage: ProcessedImage,
    imageUrl: string,
    speciesName?: string
  ): Promise<PlantAnalysis['health']> {
    // Analyse combin√©e vision + mod√®les sp√©cialis√©s
    const [visionAnalysis, mlAnalysis] = await Promise.all([
      this.analyzeHealthWithVision(imageUrl, speciesName),
      this.analyzeHealthWithML(processedImage, speciesName)
    ]);

    return this.fusionHealthResults(visionAnalysis, mlAnalysis);
  }

  private async analyzeHealthWithVision(
    imageUrl: string,
    speciesName?: string
  ): Promise<any> {
    const speciesContext = speciesName 
      ? `Cette plante est identifi√©e comme: ${speciesName}. ` 
      : '';

    const completion = await this.openaiClient.chat.completions.create({
      model: 'gpt-4-vision-preview',
      messages: [
        {
          role: 'user',
          content: [
            {
              type: 'text',
              text: `${speciesContext}Analyse la sant√© de cette plante en d√©tail:

1. √âtat g√©n√©ral de sant√© (score 0-100)
2. Probl√®mes visibles (maladies, parasites, carences)
3. Zones affect√©es avec pourcentages
4. Sympt√¥mes observ√©s
5. Causes probables
6. Urgence du traitement (low/medium/high/critical)
7. Traitements recommand√©s

Format JSON avec: overallScore, issues, affectedAreas, symptoms, treatments`
            },
            {
              type: 'image_url',
              image_url: { url: imageUrl }
            }
          ]
        }
      ],
      max_tokens: 1000
    });

    return JSON.parse(completion.choices[0].message.content || '{}');
  }

  private async analyzeHealthWithML(
    processedImage: ProcessedImage,
    speciesName?: string
  ): Promise<any> {
    // S√©lection du mod√®le selon l'esp√®ce
    const modelKey = speciesName ? `disease_${speciesName.toLowerCase()}` : 'disease_general';
    const model = this.diseaseModels.get(modelKey) || this.diseaseModels.get('disease_general');

    if (!model) {
      return { confidence: 0, diseases: [] };
    }

    // Features extraction pour le mod√®le ML
    const features = await this.extractHealthFeatures(processedImage);

    // Pr√©diction des maladies
    const predictions = await model.predict(features);

    return this.parseMLHealthResults(predictions);
  }

  private async analyzeGrowth(
    processedImage: ProcessedImage,
    imageUrl: string,
    speciesName?: string
  ): Promise<PlantAnalysis['growth']> {
    const [visionGrowth, mlGrowth] = await Promise.all([
      this.analyzeGrowthWithVision(imageUrl, speciesName),
      this.analyzeGrowthWithML(processedImage, speciesName)
    ]);

    return this.fusionGrowthResults(visionGrowth, mlGrowth);
  }

  private async analyzeGrowthWithVision(
    imageUrl: string,
    speciesName?: string
  ): Promise<any> {
    const speciesContext = speciesName 
      ? `Cette plante est: ${speciesName}. ` 
      : '';

    const completion = await this.openaiClient.chat.completions.create({
      model: 'gpt-4-vision-preview',
      messages: [
        {
          role: 'user',
          content: [
            {
              type: 'text',
              text: `${speciesContext}Analyse le stade de croissance de cette plante:

1. Stade actuel (germination, croissance, floraison, fructification, maturit√©)
2. Progression vers le stade suivant (0-100%)
3. M√©triques visuelles (hauteur approximative, nombre de feuilles/fleurs/fruits)
4. Comparaison avec croissance normale pour cette esp√®ce
5. Signes de maturation
6. Estimation biomasse

Format JSON: stage, progressPercentage, metrics, maturitySigns`
            },
            {
              type: 'image_url',
              image_url: { url: imageUrl }
            }
          ]
        }
      ],
      max_tokens: 600
    });

    return JSON.parse(completion.choices[0].message.content || '{}');
  }

  // Syst√®me de Time-lapse intelligent
  async createTimeLapse(
    imageSequence: TimelapseImage[],
    config: TimelapseConfig
  ): Promise<TimelapseResult> {
    // Tri et validation de la s√©quence
    const sortedImages = this.sortAndValidateSequence(imageSequence);
    
    // Alignement automatique des images
    const alignedImages = await this.alignImages(sortedImages);
    
    // Analyse de l'√©volution
    const evolutionAnalysis = await this.analyzeEvolution(alignedImages);
    
    // D√©tection des √©v√©nements marquants
    const keyEvents = await this.detectKeyEvents(evolutionAnalysis);
    
    // G√©n√©ration du time-lapse
    const timelapseVideo = await this.generateTimelapse(alignedImages, {
      ...config,
      keyEvents,
      evolutionData: evolutionAnalysis
    });

    // G√©n√©ration narrative
    const narrative = await this.generateEvolutionNarrative(evolutionAnalysis, keyEvents);

    return {
      videoUrl: timelapseVideo.url,
      duration: timelapseVideo.duration,
      frameCount: alignedImages.length,
      evolutionAnalysis,
      keyEvents,
      narrative,
      metadata: {
        startDate: sortedImages[0].timestamp,
        endDate: sortedImages[sortedImages.length - 1].timestamp,
        totalDays: this.calculateDaysBetween(sortedImages[0].timestamp, sortedImages[sortedImages.length - 1].timestamp)
      }
    };
  }

  private async alignImages(images: TimelapseImage[]): Promise<AlignedImage[]> {
    // D√©tection des points cl√©s sur la premi√®re image (r√©f√©rence)
    const referenceKeypoints = await this.detectKeypoints(images[0]);
    
    const alignedImages: AlignedImage[] = [{
      ...images[0],
      transform: this.identityTransform(),
      alignmentScore: 1.0
    }];

    // Alignement des images suivantes
    for (let i = 1; i < images.length; i++) {
      const currentKeypoints = await this.detectKeypoints(images[i]);
      const transform = await this.calculateTransform(referenceKeypoints, currentKeypoints);
      const alignedImage = await this.applyTransform(images[i], transform);
      
      alignedImages.push({
        ...images[i],
        transform,
        alignmentScore: this.calculateAlignmentScore(referenceKeypoints, currentKeypoints)
      });
    }

    return alignedImages;
  }

  private async analyzeEvolution(images: AlignedImage[]): Promise<EvolutionAnalysis> {
    const analysisPoints: EvolutionPoint[] = [];
    
    for (const image of images) {
      const analysis = await this.analyzeImage(
        Buffer.from(image.buffer), 
        image.metadata, 
        ['health', 'growth']
      );
      
      analysisPoints.push({
        timestamp: image.timestamp,
        healthScore: analysis.health.score,
        growthMetrics: analysis.growth.metrics,
        stage: analysis.growth.stage.name,
        issues: analysis.health.issues
      });
    }

    return {
      points: analysisPoints,
      trends: this.calculateTrends(analysisPoints),
      milestones: this.identifyMilestones(analysisPoints)
    };
  }

  private async detectKeyEvents(evolution: EvolutionAnalysis): Promise<KeyEvent[]> {
    const events: KeyEvent[] = [];

    for (let i = 1; i < evolution.points.length; i++) {
      const current = evolution.points[i];
      const previous = evolution.points[i - 1];

      // D√©tection changement de stade
      if (current.stage !== previous.stage) {
        events.push({
          type: 'stage_change',
          timestamp: current.timestamp,
          description: `Passage de ${previous.stage} √† ${current.stage}`,
          significance: 'high'
        });
      }

      // D√©tection croissance rapide
      const growthRate = this.calculateGrowthRate(previous, current);
      if (growthRate > 1.5) { // 150% du taux normal
        events.push({
          type: 'rapid_growth',
          timestamp: current.timestamp,
          description: `Croissance acc√©l√©r√©e d√©tect√©e`,
          significance: 'medium'
        });
      }

      // D√©tection probl√®mes de sant√©
      const healthDrop = previous.healthScore - current.healthScore;
      if (healthDrop > 20) {
        events.push({
          type: 'health_issue',
          timestamp: current.timestamp,
          description: `D√©t√©rioration de sant√©: -${healthDrop} points`,
          significance: 'high'
        });
      }
    }

    return events.sort((a, b) => b.significance.localeCompare(a.significance));
  }

  private async generateEvolutionNarrative(
    evolution: EvolutionAnalysis,
    keyEvents: KeyEvent[]
  ): Promise<string> {
    const prompt = `G√©n√®re un r√©cit captivant de l'√©volution de cette plante bas√© sur les donn√©es suivantes:

Donn√©es d'√©volution:
${JSON.stringify(evolution, null, 2)}

√âv√©nements cl√©s:
${JSON.stringify(keyEvents, null, 2)}

√âcris un r√©cit de 200-300 mots qui:
1. Raconte l'histoire de croissance de cette plante
2. Met en valeur les moments marquants
3. Explique les d√©fis surmont√©s
4. Utilise un ton passionnant et √©ducatif
5. Inclut des d√©tails techniques accessibles`;

    const completion = await this.openaiClient.chat.completions.create({
      model: 'gpt-4-turbo-preview',
      messages: [{ role: 'user', content: prompt }],
      max_tokens: 500
    });

    return completion.choices[0].message.content || '';
  }

  // Training et am√©lioration des mod√®les
  async trainDiseaseModel(
    speciesName: string,
    trainingData: DiseaseTrainingData[]
  ): Promise<void> {
    const modelId = `disease_${speciesName.toLowerCase()}`;
    
    // Pr√©paration des donn√©es d'entra√Ænement
    const processedData = await this.preprocessTrainingData(trainingData);
    
    // Configuration du mod√®le
    const modelConfig = {
      architecture: 'CNN',
      layers: [
        { type: 'conv2d', filters: 32, kernelSize: 3 },
        { type: 'maxpool2d', poolSize: 2 },
        { type: 'conv2d', filters: 64, kernelSize: 3 },
        { type: 'maxpool2d', poolSize: 2 },
        { type: 'flatten' },
        { type: 'dense', units: 128, activation: 'relu' },
        { type: 'dropout', rate: 0.3 },
        { type: 'dense', units: trainingData[0].diseases.length, activation: 'softmax' }
      ],
      optimizer: 'adam',
      loss: 'categorical_crossentropy',
      metrics: ['accuracy']
    };

    // Entra√Ænement
    const model = await this.mlService.trainModel(modelConfig, processedData);
    
    // Validation
    const validationResult = await this.validateModel(model, processedData.validationSet);
    
    if (validationResult.accuracy >= 0.8) {
      // Sauvegarde du mod√®le
      await this.saveModel(model, modelId);
      this.diseaseModels.set(modelId, model);
      
      console.log(`Disease model for ${speciesName} trained successfully. Accuracy: ${validationResult.accuracy}`);
    } else {
      throw new Error(`Model accuracy too low: ${validationResult.accuracy}`);
    }
  }
}
```

### Interface Computer Vision

```typescript
// src/app/vision/page.tsx
export default function ComputerVisionPage() {
  const [analysisResult, setAnalysisResult] = useState<PlantAnalysis | null>(null);
  const [isAnalyzing, setIsAnalyzing] = useState(false);
  const [activeTab, setActiveTab] = useState<'analysis' | 'timelapse' | 'history'>('analysis');

  return (
    <div className="container mx-auto py-6">
      <div className="max-w-6xl mx-auto">
        <div className="flex justify-between items-center mb-6">
          <div>
            <h1 className="text-2xl font-bold">üîç Vision Intelligente</h1>
            <p className="text-gray-600">Analyse automatique de vos plantes</p>
          </div>
        </div>

        <div className="grid grid-cols-12 gap-6">
          {/* Upload et analyse */}
          <div className="col-span-5">
            <Card className="p-6">
              <h3 className="text-lg font-semibold mb-4">Analyser une Image</h3>
              
              <PhotoUpload
                onUpload={handlePhotoAnalysis}
                disabled={isAnalyzing}
              />

              {isAnalyzing && (
                <div className="mt-4 p-4 bg-blue-50 rounded-lg">
                  <div className="flex items-center space-x-2">
                    <Loader2Icon className="w-4 h-4 animate-spin" />
                    <span className="text-sm">Analyse en cours...</span>
                  </div>
                  <Progress value={analysisProgress} className="mt-2" />
                </div>
              )}
            </Card>

            {analysisResult && (
              <Card className="p-6 mt-4">
                <h3 className="text-lg font-semibold mb-4">Recommandations</h3>
                <RecommendationsList recommendations={analysisResult.recommendations} />
              </Card>
            )}
          </div>

          {/* R√©sultats */}
          <div className="col-span-7">
            {analysisResult ? (
              <div className="space-y-4">
                <SpeciesIdentificationCard species={analysisResult.species} />
                <HealthAnalysisCard health={analysisResult.health} />
                <GrowthAnalysisCard growth={analysisResult.growth} />
              </div>
            ) : (
              <Card className="p-8 text-center">
                <CameraIcon className="w-16 h-16 mx-auto text-gray-400 mb-4" />
                <h3 className="text-lg font-medium mb-2">Pr√™t √† analyser</h3>
                <p className="text-gray-600">
                  Uploadez une photo pour commencer l'analyse automatique
                </p>
              </Card>
            )}
          </div>
        </div>
      </div>
    </div>
  );
}

const SpeciesIdentificationCard = ({ species }: { species?: PlantAnalysis['species'] }) => {
  if (!species) return null;

  return (
    <Card className="p-4">
      <div className="flex items-start space-x-3">
        <div className="w-10 h-10 bg-green-100 rounded-lg flex items-center justify-center">
          <LeafIcon className="w-5 h-5 text-green-600" />
        </div>
        
        <div className="flex-1">
          <div className="flex items-center justify-between mb-2">
            <h3 className="font-semibold">Identification</h3>
            <Badge variant="outline">
              {Math.round(species.confidence * 100)}% confiance
            </Badge>
          </div>
          
          <p className="font-medium">{species.name}</p>
          <p className="text-sm text-gray-600">{species.family}</p>
          
          <div className="mt-3 space-y-1">
            <p className="text-xs font-medium text-gray-700">Caract√©ristiques:</p>
            {species.characteristics.slice(0, 3).map((char, index) => (
              <p key={index} className="text-xs text-gray-600">‚Ä¢ {char}</p>
            ))}
          </div>
        </div>
      </div>
    </Card>
  );
};

const HealthAnalysisCard = ({ health }: { health: PlantAnalysis['health'] }) => {
  const healthColor = {
    excellent: 'text-green-600',
    good: 'text-green-500',
    fair: 'text-yellow-500',
    poor: 'text-orange-500',
    critical: 'text-red-500'
  }[health.overall];

  return (
    <Card className="p-4">
      <div className="flex items-start space-x-3">
        <div className="w-10 h-10 bg-blue-100 rounded-lg flex items-center justify-center">
          <HeartIcon className="w-5 h-5 text-blue-600" />
        </div>
        
        <div className="flex-1">
          <div className="flex items-center justify-between mb-2">
            <h3 className="font-semibold">√âtat de Sant√©</h3>
            <div className="text-right">
              <div className={`font-bold ${healthColor} capitalize`}>
                {health.overall}
              </div>
              <div className="text-sm text-gray-600">
                {health.score}/100
              </div>
            </div>
          </div>

          <Progress value={health.score} className="mb-3" />

          {health.issues.length > 0 && (
            <div className="space-y-2">
              <p className="text-xs font-medium text-gray-700">Probl√®mes d√©tect√©s:</p>
              {health.issues.map((issue, index) => (
                <HealthIssueItem key={index} issue={issue} />
              ))}
            </div>
          )}
        </div>
      </div>
    </Card>
  );
};

const HealthIssueItem = ({ issue }: { issue: HealthIssue }) => {
  const severityColor = {
    low: 'text-yellow-600 bg-yellow-50',
    medium: 'text-orange-600 bg-orange-50',
    high: 'text-red-600 bg-red-50',
    critical: 'text-red-700 bg-red-100'
  }[issue.severity];

  return (
    <div className={`p-2 rounded text-xs ${severityColor}`}>
      <div className="flex justify-between items-center">
        <span className="font-medium">{issue.name}</span>
        <span className="text-xs">
          {Math.round(issue.confidence * 100)}% confiance
        </span>
      </div>
      <p className="mt-1 text-xs opacity-80">
        {issue.symptoms.join(', ')}
      </p>
    </div>
  );
};

const TimelapseCreator = () => {
  const [imageSequence, setImageSequence] = useState<File[]>([]);
  const [isGenerating, setIsGenerating] = useState(false);
  const [timelapseResult, setTimelapseResult] = useState<TimelapseResult | null>(null);

  return (
    <div className="space-y-6">
      <Card className="p-6">
        <h3 className="text-lg font-semibold mb-4">Cr√©er un Time-lapse</h3>
        
        <div className="space-y-4">
          <div>
            <label className="block text-sm font-medium mb-2">
              S√©quence d'images ({imageSequence.length} photos)
            </label>
            <MultiImageUpload
              images={imageSequence}
              onChange={setImageSequence}
              maxImages={100}
            />
          </div>

          <div className="grid grid-cols-2 gap-4">
            <div>
              <label className="block text-sm font-medium mb-1">Dur√©e finale</label>
              <Select>
                <SelectTrigger>
                  <SelectValue placeholder="S√©lectionner" />
                </SelectTrigger>
                <SelectContent>
                  <SelectItem value="5s">5 secondes</SelectItem>
                  <SelectItem value="10s">10 secondes</SelectItem>
                  <SelectItem value="15s">15 secondes</SelectItem>
                  <SelectItem value="30s">30 secondes</SelectItem>
                </SelectContent>
              </Select>
            </div>
            
            <div>
              <label className="block text-sm font-medium mb-1">Qualit√©</label>
              <Select>
                <SelectTrigger>
                  <SelectValue placeholder="HD" />
                </SelectTrigger>
                <SelectContent>
                  <SelectItem value="720p">HD (720p)</SelectItem>
                  <SelectItem value="1080p">Full HD (1080p)</SelectItem>
                  <SelectItem value="4k">4K</SelectItem>
                </SelectContent>
              </Select>
            </div>
          </div>

          <Button 
            onClick={handleGenerateTimelapse}
            disabled={imageSequence.length < 5 || isGenerating}
            className="w-full"
          >
            {isGenerating ? (
              <>
                <Loader2Icon className="w-4 h-4 mr-2 animate-spin" />
                G√©n√©ration en cours...
              </>
            ) : (
              <>
                <VideoIcon className="w-4 h-4 mr-2" />
                G√©n√©rer le Time-lapse
              </>
            )}
          </Button>
        </div>
      </Card>

      {timelapseResult && (
        <TimelapseResultCard result={timelapseResult} />
      )}
    </div>
  );
};
```

## Crit√®res d'Acceptation Techniques

### Reconnaissance Maladies
- [ ] 20+ maladies communes identifi√©es >80% pr√©cision
- [ ] Localisation zones affect√©es avec bounding boxes
- [ ] Diagnostic avec niveau de confiance calibr√©
- [ ] Recommandations traitement contextualis√©es
- [ ] Base de donn√©es sympt√¥mes extensible

### Suivi Croissance
- [ ] Reconnaissance stades par esp√®ce v√©g√©tale
- [ ] Mesures automatiques (hauteur, volume, comptage)
- [ ] Comparaison avec standards croissance
- [ ] D√©tection anomalies et retards
- [ ] Historique √©volution avec m√©triques

### Time-lapse Intelligent
- [ ] Alignement automatique s√©quences photos
- [ ] G√©n√©ration vid√©o haute qualit√© (jusqu'√† 4K)
- [ ] D√©tection √©v√©nements marquants automatique
- [ ] Analyse narrative de l'√©volution
- [ ] Export avec m√©tadonn√©es enrichies

## Couverture Exigences Architecture

- **EXG-005.1** : Computer Vision avanc√©e avec ML
- **EXG-005.2** : Mod√®les d'IA sp√©cialis√©s entra√Ænables
- **EXG-002.1** : Stockage images et analyses
- **EXG-003.1** : Interface responsive vision
- **EXG-009.1** : M√©triques pr√©cision et co√ªts

## Tests d'Acceptation

```typescript
// tests/integration/computer-vision.test.ts
describe('Computer Vision Service', () => {
  test('identification esp√®ce avec confiance', async () => {
    const imageBuffer = await fs.readFile('test-tomato-plant.jpg');
    const analysis = await visionService.analyzeImage(imageBuffer, testMetadata, ['species']);
    
    expect(analysis.species?.name).toContain('Solanum');
    expect(analysis.species?.confidence).toBeGreaterThan(0.8);
  });

  test('d√©tection maladie avec localisation', async () => {
    const diseaseImageBuffer = await fs.readFile('test-plant-disease.jpg');
    const analysis = await visionService.analyzeImage(diseaseImageBuffer, testMetadata, ['health']);
    
    expect(analysis.health.issues).toHaveLength(greaterThan(0));
    expect(analysis.health.issues[0].affectedAreas).toBeDefined();
    expect(analysis.health.issues[0].confidence).toBeGreaterThan(0.7);
  });

  test('g√©n√©ration time-lapse avec √©v√©nements', async () => {
    const imageSequence = await loadTestImageSequence();
    const timelapse = await visionService.createTimeLapse(imageSequence, {
      duration: 10,
      fps: 30,
      quality: '1080p'
    });
    
    expect(timelapse.videoUrl).toBeDefined();
    expect(timelapse.keyEvents.length).toBeGreaterThan(0);
    expect(timelapse.narrative).toContain('√©volution');
  });
});
```

Cette sp√©cification couvre un syst√®me complet de computer vision pour diagnostic automatique, suivi de croissance et cr√©ation de time-lapse intelligents.